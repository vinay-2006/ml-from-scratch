{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01f198e",
   "metadata": {},
   "source": [
    "# Model Evaluation — From Scratch\n",
    "\n",
    "Up to Day 6, the focus was on **training models**.\n",
    "Training alone does not tell us whether a model is useful.\n",
    "\n",
    "This notebook focuses on **evaluation**:\n",
    "- how to test models correctly\n",
    "- how to compute metrics from scratch\n",
    "- how to diagnose failure cases\n",
    "\n",
    "The goal is not to say *that* a model is bad,\n",
    "but to explain **why** it is bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4d3e2",
   "metadata": {},
   "source": [
    "## Why Evaluation Matters\n",
    "\n",
    "A model can:\n",
    "- perform extremely well on training data\n",
    "- completely fail on unseen data\n",
    "\n",
    "Evaluation answers a different question than training:\n",
    "\n",
    "- Training: Can the model fit the data?\n",
    "- Evaluation: Can the model generalize?\n",
    "\n",
    "These questions must never be mixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b39ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608f401",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We reuse a binary classification dataset so that:\n",
    "- failure cases can be inspected\n",
    "- linear and logistic models can be compared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d576e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "X0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
    "y0 = np.zeros(n_samples // 2)\n",
    "\n",
    "X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "y1 = np.ones(n_samples // 2)\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace44b79",
   "metadata": {},
   "source": [
    "## Manual Train / Test Split\n",
    "\n",
    "The dataset is shuffled and split manually.\n",
    "\n",
    "The test set must never be used during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac6f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(X))\n",
    "split = int(0.8 * len(X))\n",
    "\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b93b2",
   "metadata": {},
   "source": [
    "## Confusion Matrix (From Scratch)\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "- True Positive  (TP)\n",
    "- True Negative  (TN)\n",
    "- False Positive (FP)\n",
    "- False Negative (FN)\n",
    "\n",
    "All evaluation metrics are derived from these counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf78af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6413b2a",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (From Scratch)\n",
    "\n",
    "Accuracy, Precision, and Recall measure **different aspects** of performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e822d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN) if (TP + FN) > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73369934",
   "metadata": {},
   "source": [
    "## Model 1 — Linear Regression Used as a Classifier (Bad Baseline)\n",
    "\n",
    "We intentionally misuse linear regression as a classifier\n",
    "by thresholding its outputs.\n",
    "\n",
    "This serves as a baseline to show **what not to do**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8a01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression\n",
    "X_aug = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
    "w_lin = np.linalg.inv(X_aug.T @ X_aug) @ X_aug.T @ y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9632bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "X_test_aug = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "y_lin_scores = X_test_aug @ w_lin\n",
    "y_lin_pred = (y_lin_scores >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8ed88",
   "metadata": {},
   "source": [
    "## Model 2 — Logistic Regression (Correct Model)\n",
    "\n",
    "This model outputs probabilities and is trained using log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2125e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714de46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "w = np.zeros(X_train.shape[1])\n",
    "b = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "for _ in range(300):\n",
    "    z = X_train @ w + b\n",
    "    y_hat = sigmoid(z)\n",
    "\n",
    "    dw = (1 / len(y_train)) * X_train.T @ (y_hat - y_train)\n",
    "    db = (1 / len(y_train)) * np.sum(y_hat - y_train)\n",
    "\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a6ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "y_log_probs = sigmoid(X_test @ w + b)\n",
    "y_log_pred = (y_log_probs >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac78ab",
   "metadata": {},
   "source": [
    "## Metric Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff6793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (as classifier)\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Linear model metrics\n",
    "TP_l, TN_l, FP_l, FN_l = confusion_matrix(y_test, y_lin_pred)\n",
    "\n",
    "# Logistic model metrics\n",
    "TP_g, TN_g, FP_g, FN_g = confusion_matrix(y_test, y_log_pred)\n",
    "\n",
    "print(\"Linear Regression (as classifier)\")\n",
    "print(\"Accuracy:\", accuracy(TP_l, TN_l, FP_l, FN_l))\n",
    "print(\"Precision:\", precision(TP_l, FP_l))\n",
    "print(\"Recall:\", recall(TP_l, FN_l))\n",
    "\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(\"Accuracy:\", accuracy(TP_g, TN_g, FP_g, FN_g))\n",
    "print(\"Precision:\", precision(TP_g, FP_g))\n",
    "print(\"Recall:\", recall(TP_g, FN_g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7177a",
   "metadata": {},
   "source": [
    "## Why Accuracy Alone Is Dangerous\n",
    "\n",
    "Accuracy hides **what kind of mistakes** the model is making.\n",
    "\n",
    "Two models can have the same accuracy:\n",
    "- one misses critical positives\n",
    "- the other makes harmless false alarms\n",
    "\n",
    "Accuracy cannot distinguish between these cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc77e3",
   "metadata": {},
   "source": [
    "## Failure-Case Analysis\n",
    "\n",
    "We now inspect **specific failures** made by the models.\n",
    "This is where evaluation becomes diagnostic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8e1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify false negatives and false positives for logistic regression\n",
    "false_negatives = np.where((y_test == 1) & (y_log_pred == 0))[0]\n",
    "false_positives = np.where((y_test == 0) & (y_log_pred == 1))[0]\n",
    "\n",
    "false_negatives[:5], false_positives[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728baf6",
   "metadata": {},
   "source": [
    "False negatives:\n",
    "- dangerous when positives must not be missed\n",
    "\n",
    "False positives:\n",
    "- costly when false alarms matter\n",
    "\n",
    "Which error matters more depends on the application.\n",
    "Metrics must be chosen accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1dddfc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Evaluation must use unseen data\n",
    "- Metrics are derived from confusion counts\n",
    "- Accuracy alone can be misleading\n",
    "- Precision and recall expose different failure modes\n",
    "- A model is judged by *how* it fails, not just *how often*\n",
    "\n",
    "This completes Week B: ML from scratch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
